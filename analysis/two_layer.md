## Two-Layer Attention-Only Model

### Q, K and V Composition

![QKV Composition](assets/induction_heads.png)
We see from the array on the right that layer-0 head 11 is a previous token head. Layer-1 heads 3, 7, and 9 k-compose with this previous token head.
